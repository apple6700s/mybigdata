## MIG数据处理流程

### 1. 数据源

##### 1. 微博内容 (Rhino: WeiboConsume, Banyan: WeiboContent)

任务触发方式：取预先圈定的活跃用户（目前1810w）**每天**定期全量按uid爬取最新微博

目前更新频率：1500w 微博/天

任务处理流程：kafka获取WeiboStatus对象（包含Weibo和User），Weibo和User都覆盖写(upsert) HBase，Weibo同时更新ES。User 仅当ES不存在的时候才创建doc。

注：上述方法会导致可能一个用户的基本信息（比如粉丝数、微博数等）会滞后两三个月（原因请看4和5）才更新，导致ES和HBase数据不同步。这个坑已提醒jincheng和sugan。

##### 2. 微博评论 (Rhino: WeiboComment, Banyan: WeiboComment)

任务触发方式：提交接口触发

任务处理流程：kafka获取json对象，upsert HBase，同时刷ES（评论内容字段只存索引不存source）。

##### 3. 转评赞 (Rhino: WeiboUpdate, Banyan: WeiboStat)

任务触发方式： 提交接口触发

任务处理流程：kafka获取json对象，HBase 覆盖写总数，每天的差值写ES（MIG特殊需求）。

##### 4. 高级接口 （未做, Banyan: WeiboAdvUserInfo）

更新用户的公司、学校、生日等需要特殊接口获取的信息

任务触发方式：爬虫组每天不同的100w用户更新，3个月一轮

任务处理流程：kafka，流式写HBase，列表字段不合并直接覆盖，每天定时100w用户批量刷ES（与6同一个写入操作）

##### 5. 关注列表 （未做, Banyan: WeiboFollow）

更新用户的关注人

任务触发方式：爬虫组每天不同的100w用户更新，3个月一轮

任务处理流程：kafka，流式写HBase，每天定时100w用户批量刷ES

##### 6. LongText 

包括贴吧、论坛、新闻，新闻评论不抓取，三者存储schema基本一致。目前有2w个url网站种子，每天更新新的网页。用cat_id字段区分长文本种类。

HBase: dt.rhino.sys.common ; ES: dt_rhino_newsforum_index

任务处理流程：kafka，直接写HBase和ES，其中ES内容相关的字段只存索引（content和mainPost，content为主贴拼凑回复，mainPost为主贴；新闻只有content，mainPost为空）。评论或回帖则是任务接口提交触发。

### 2. Durian旧库

工作进度：内容迁移完成，linyong在迁移用户，据说也快了。接口基本相当于未迁移，需和yangfei对接。

底层sdk核心迁移类：durian-commons-core的DurianSearcher

### 3. 接口

目前提供了mig的sdk接口和durian的sdk/http接口。mig-sdk方面基本都是agg基础接口和一些针对用户、微博等的search接口。

目前缺乏任务状态监控和数据处理监控

### 其他

1. 需注意的ES写入坑：所有涉及不存储source的doc在update的时候，没有source的field会丢失，比如comment或content或follower等，这部分需要重新读HBase去重新写
2. 目前的父子文档：user->weibo , weibo->comment, 主贴->回帖

