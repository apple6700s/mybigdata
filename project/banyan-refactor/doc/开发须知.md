## 域名相关信息

alps和todi的机器信息可于跳板机 `/etc/hosts` 中获取 。跳板机 `g apls1` 可以gondor用户登录。

kafka.broker.list=alps18:6667,alps3:6667,alps35:6667,alps51:6667,alps8:6667

spark.master.url=spark://alps62:7077

zk.conn=alps17:2181,alps2:2181,alps34:2181,alps50:2181,alps59:2181

es.hosts=alps61:9300,alps62:9300,alps63:9300,todi1:9300,todi2:9300,todi3:9300,todi4:9300,todi5:9300,todi6:9300,todi7:9300,todi8:9300,todi9:9300,todi10:9300,todi11:9300,todi12:9300,todi13:9300,todi14:9300,todi16:9300,todi17:9300,todi18:9300,todi19:9300,todi20:9300,todi21:9300,todi22:9300,todi23:9300,todi24:9300,todi25:9300,todi26:9300,todi27:9300,todi28:9300,todi29:9300,todi30:9300,todi31:9300,todi32:9300,todi33:9300,todi34:9300,todi35:9300,todi36:9300,todi37:9300,todi38:9300,todi39:9300,todi40:9300,todi41:9300,todi42:9300,todi43:9300,todi44:9300,todi45:9300,todi46:9300,todi47:9300,todi48:9300,todi1:9301,todi2:9301,todi3:9301,todi4:9301,todi5:9301,todi6:9301,todi7:9301,todi8:9301,todi9:9301,todi10:9301,todi11:9301,todi12:9301,todi13:9301,todi14:9301,todi16:9301,todi17:9301,todi18:9301,todi19:9301,todi20:9301,todi21:9301,todi22:9301,todi23:9301,todi24:9301,todi25:9301,todi26:9301,todi27:9301,todi28:9301,todi29:9301,todi30:9301,todi31:9301,todi32:9301,todi33:9301,todi34:9301,todi35:9301,todi36:9301,todi37:9301,todi38:9301,todi39:9301,todi40:9301,todi41:9301,todi42:9301,todi43:9301,todi44:9301,todi45:9301,todi46:9301,todi47:9301,todi48:9301

es.web.url=http://es-rhino.datatub.com

## 一些测试命令

kafka consumer 测试：（可于跳板机执行）

```bash
/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh  --zookeeper alps34:2181 --topic topic_rhino_weibo_count --from-beginning
/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --zookeeper alps34:2181
```


## 添加一个新数据库大致流程

1. 与爬虫及业务方确认好相关的schema结构，一般简单的主要是hbase表（用phoenix sql维护），有查询需求的还可能会有es表。rhino的kafka需记录json结构、topic和group，刷es的kafka需创建新的topic和group。
2. 将确认好的schema文档更新到 `doc/schema` 下，相应结构请参考旧的文件结构。
3. 与爬虫组确认好数据对接方式，一般是kafka实时或hbase定时刷
4. 开发对应的RhinoConsumer，实时的请注意做性能调整（spark cores, repartition, kafka和spark配置等）
5. 开发对应的PhoenixWriter，请注意不要每次结束都flush（让其自动flush）。请注意仔细计算byte大小大致确认批量数，一般长文本类为2000，微博类为10000.
6. 开发对应的ESWriter，请注意不要每次结束都flush（让其自动flush）。请注意仔细计算byte大小大致确认批量数，一般长文本类为2000，微博类为10000.
7. 开发对应的DocMapper，一般至少有 爬虫结果到HBase的转换（比如JSONObjectDocMapper）、Hbase到es的转换（ParamsDocMapper）。请尽量保证DocMapper继承自`commons-core`的`com.datastory.banyan.doc`下的其中一个类。
8. 如果有分析需求，请参考其他项目建analyz包并开发相应的analyzer。
9. 此时写HBase流程基本完成。
10. 开发相应的ToESConsumer和ToESConsumerProcessor
11. 开发相应的HBaseReader
12. PhoenixWriter 加个继承自 `com.datastory.banyan.kafka.PKSpoutKafkaProducer`的钩子，保证写hbase后会自动将写成功的pk发送到kafka。
13. 此时写ES流程基本完成。
